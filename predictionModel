#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
import scipy
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import cross_validate, cross_val_predict
from sklearn import model_selection
from sklearn.metrics import mean_squared_log_error
from sklearn.model_selection import KFold
from pprint import pprint
from sklearn.metrics import mean_absolute_error


# In[2]:


def evaluate(model, X_test, y_test):
    predictions = model.predict(X_test)
    errors = abs(predictions - y_test)
    mape = 100 * np.mean(errors / y_test)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    
    return accuracy


# In[3]:


data = pd.read_csv('HousePriceDataset2.csv')


# In[4]:


data.describe()


# In[5]:


def get_mae(max_depth,train_X,val_X,train_y,val_y):
    model=RandomForestRegressor(max_depth=max_depth,random_state=0)
    model.fit(train_X,train_y)
    preds_val=model.predict(val_X)
    mae=mean_absolute_error(val_y,preds_val)
    return(mae)


# In[12]:


for max_leaf_nodes in [5, 50, 500,700, 5000,7000]:
    my_mae = get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)
    print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))


# In[24]:


for max_depth in [5,8,10,12,15,20,30,60,70,80]:
    my_mae=get_mae(max_depth, X_train, X_test, y_train, y_test)
    print("Max depth: %d  \t\t Mean Absolute Error:  %d" %(max_depth, my_mae))


# In[25]:


def get_mae(min_samples_split,train_X,val_X,train_y,val_y):
    model=RandomForestRegressor(min_samples_split=min_samples_split,random_state=0)
    model.fit(train_X,train_y)
    preds_val=model.predict(val_X)
    mae=mean_absolute_error(val_y,preds_val)
    return(mae)


# In[26]:


for min_samples_split in [1.0,2,3,4,5,6,7,8,9,10,12,14,20,30]:
    my_mae=get_mae(min_samples_split, X_train, X_test, y_train, y_test)
    print("Mean samples split: %d  \t\t Mean Absolute Error:  %d" %(min_samples_split, my_mae))


# In[27]:


def get_mae(min_samples_leaf,train_X,val_X,train_y,val_y):
    model=RandomForestRegressor(min_samples_leaf=min_samples_leaf,random_state=0)
    model.fit(train_X,train_y)
    preds_val=model.predict(val_X)
    mae=mean_absolute_error(val_y,preds_val)
    return(mae)


# In[28]:


for min_samples_leaf in [1,2,3,4,5,10,15,20,30,40,50,60,70]:
    my_mae=get_mae(min_samples_leaf, X_train, X_test, y_train, y_test)
    print("min samples leaf: %d  \t\t Mean Absolute Error:  %d" %(min_samples_leaf, my_mae))


# In[29]:


def get_mae(max_features,train_X,val_X,train_y,val_y):
    model=RandomForestRegressor(max_features=max_features,random_state=0)
    model.fit(train_X,train_y)
    preds_val=model.predict(val_X)
    mae=mean_absolute_error(val_y,preds_val)
    return(mae)


# In[30]:


for max_features in [1,2,3,4,5,6,7,8]:
    my_mae=get_mae(max_features, X_train, X_test, y_train, y_test)
    print("max features: %d  \t\t Mean Absolute Error:  %d" %(max_features, my_mae))


# In[31]:


def get_mae(n_estimators,train_X,val_X,train_y,val_y):
    model=RandomForestRegressor(n_estimators=n_estimators,random_state=0)
    model.fit(train_X,train_y)
    preds_val=model.predict(val_X)
    mae=mean_absolute_error(val_y,preds_val)
    return(mae)


# In[32]:


for n_estimators in [10,20,30,50,80,100,200,300,400,500,600,700,800,900,1000]:
    my_mae=get_mae(n_estimators, X_train, X_test, y_train, y_test)
    print("n_estimators: %d  \t\t Mean Absolute Error:  %d" %(n_estimators, my_mae))


# In[ ]:





# # data preprocessing

# In[7]:


label_enc = LabelEncoder()
data.iloc[:,0] = label_enc.fit_transform(data.iloc[:,0])


# In[8]:


X = data.drop('Price', axis=1)
y = data['Price']


# In[9]:


X_list = list(X.columns)


# In[ ]:





# In[10]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)


# In[11]:


y_test.describe()


# In[13]:


X_train


# # creating the baseline model

# In[38]:


from sklearn.ensemble import RandomForestRegressor
rt_model=RandomForestRegressor(random_state=42)
rt_model.fit(X_train,y_train)


# In[39]:


importance = rt_model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))


# In[77]:


base_model_accuracy=evaluate(rt_model,X_test,y_test)


# In[78]:


test_pred_base = rt_model.predict(X_test)
train_pred_base = rt_model.predict(X_train)


# In[79]:


r2_score(y_test,test_pred_base)


# In[80]:


mse = mean_squared_error(y_test,test_pred_base)
rmse = np.sqrt(mse)
rmse


# In[81]:


mean_absolute_error(y_test, test_pred_base)


# In[82]:


pprint(rt_model.get_params())


# In[65]:


y_test = np.random.rand(100) # Random Data
test_pred_base = y_test + np.random.rand(100)*0.1 # Random Data

r_squared = 0.608
rmse = 41863469.49
plt.scatter(y_test,test_pred_base)
plt.xlabel('Actual values')
plt.ylabel('Predicted values')

plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, test_pred_base, 1))(np.unique(y_test)))

plt.text(0.6, 0.5,'R-squared = %0.2f' % r_squared)
plt.show()


# # randomised search

# In[125]:


from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(8, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)


# In[126]:


# Use the random grid to search for best hyperparameters
rf = RandomForestRegressor()
# Random search of parameters, using 3 fold cross validation, 
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, 
                               cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, y_train)


# In[127]:


rf_random.best_params_


# In[128]:


random_model_accuracy=evaluate(rf_random,X_test,y_test)


# In[129]:


test_pred_random = rf_random.predict(X_test)
train_pred_random = rf_random.predict(X_train)


# In[130]:


y_test.describe()


# In[131]:


r2_score(y_test,test_pred_random)


# In[36]:


mse = mean_squared_error(y_test,test_pred_random)
rmse = np.sqrt(mse)
rmse


# In[37]:


mean_absolute_error(y_test, test_pred_random)


# # grid search with cross validation

# In[85]:


from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search 
param_grid = {
    'bootstrap': [True],
    'max_depth': [11],
    'max_features': [8],
    'min_samples_leaf': [1],
    'min_samples_split': [12],
    'n_estimators': [300],
    'max_leaf_nodes':[700]    
}

# Create a based model
rf = RandomForestRegressor(random_state = 0)
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv =3, n_jobs = 4, verbose = 2, return_train_score=True)


# In[86]:


# Fit the grid search to the data
grid_search.fit(X_train, y_train)


# In[87]:


grid_search.best_params_


# In[88]:


grid_accuracy=evaluate(grid_search,X_test,y_test)


# In[89]:


test_pred_grid = grid_search.predict(X_test)
train_pred_grid = grid_search.predict(X_train)


# In[90]:


r2_score(y_test,test_pred_grid)


# In[91]:


mse = mean_squared_error(y_test,test_pred_random)
rmse = np.sqrt(mse)
rmse


# In[92]:


mean_absolute_error(y_test, test_pred_grid)


# In[93]:


r2_score(y_train,train_pred_grid)


# In[94]:


y_test = np.random.rand(100) # Random Data
test_pred_grid = y_test + np.random.rand(100)*0.1 # Random Data

r_squared = 0.724
rmse = 41863469.49
plt.scatter(y_test,test_pred_grid)
plt.xlabel('Actual values')
plt.ylabel('Predicted values')

plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, test_pred_grid, 1))(np.unique(y_test)))

plt.text(0.6, 0.5,'R-squared = %0.2f' % r_squared)
plt.show()


# In[144]:


# Training predictions (to demonstrate overfitting)
train_rf_predictions = grid_search.predict(X_train)
train_rf_probs = grid_search.predict(X_train)

# Testing predictions (to determine performance)
rf_predictions = grid_search.predict(X_test)
rf_probs = grid_search.predict(X_test)


# In[148]:


print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit


def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):

    if axes is None:
        _, axes = plt.subplots(1, 3, figsize=(20, 10))

    axes[0].set_title(title)
    if ylim is not None:
        axes[0].set_ylim(*ylim)
    axes[0].set_xlabel("Training samples")
    axes[0].set_ylabel("Score")

    train_sizes, train_scores, test_scores, fit_times, _ =         learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes,
                       return_times=True)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fit_times_mean = np.mean(fit_times, axis=1)
    fit_times_std = np.std(fit_times, axis=1)

    # Plot learning curve
    axes[0].grid()
    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes[0].plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
    axes[0].plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")
    axes[0].legend(loc="best")

    # Plot n_samples vs fit_times
    axes[1].grid()
    axes[1].plot(train_sizes, fit_times_mean, 'o-')
    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,
                         fit_times_mean + fit_times_std, alpha=0.1)
    axes[1].set_xlabel("Training samples")
    axes[1].set_ylabel("Time")
    axes[1].set_title("Performance of the model")

    # Plot fit_time vs score
    axes[2].grid()
    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')
    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1)
    axes[2].set_xlabel("time")
    axes[2].set_ylabel("Score")
    axes[2].set_title("Performance of the model")

    return plt


fig, axes = plt.subplots(3, 2, figsize=(10, 15))

X, y = load_digits(return_X_y=True)

title = "Learning Curves (House prediction model)"
# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
rf = RandomForestRegressor(random_state = 0)

estimator = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)
plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),
                    cv=cv, n_jobs=4)








plt.show()


# In[147]:


from joblib import dump
dump(grid_search, 'filename.joblib') 


# In[ ]:




